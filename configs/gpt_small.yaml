model:
  name: gpt2-small
  vocab_size: 50257
  n_layers: 12
  n_heads: 12
  d_model: 768
  dff: 3072
  dropout_rate: 0.1
  seq_len: 256     # 1024 in original GPT-2-small paper
  qkv_bias: false

training:
  batch_size: 32
  epochs: 10
  learning_rate: 3e-4
  optimizer: adamw
  weight_decay: 0.01
  gradient_clip: 1.0
  device: auto
  save_dir: checkpoints/gpt2_small

data:
  train_path: data/train.txt
  val_path: data/val.txt
  block_size: 1024

logging:
  log_interval: 100
  save_interval: 1000
  output_dir: outputs/gpt2_small

device: cuda
seed: 42